{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><span style=\"color:black; font-family:Times New Roman; font-size:3em;\"> Cervix type classification </span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style=\"text-align:center\"><span style=\"color:blue; font-family:Times New Roman; font-size:3em;\"> Introduction </span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;padding-left:3em;padding-right:3em;\"> In this project, I developed Machine Learning algorithms to classify women cervix types based on cervical images provided by Kaggle (https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening/data). The goal of the Kaggle competition ‘Cervical Cancer Screening’ is to identify a woman’s cervix type based on images. Correctly identifying a woman’s cervix type can provide an effective treatment for cervical cancer. There are three different types of cervix in this dataset, which are all considered as ‘not cancerous’.  A cervix type can be classified by its transformation zone location which is not easily identified by healthcare providers. Therefore, an algorithm-aided decision is needed to improve the efficiency of classifying cervix types. \n",
    "<br \\>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "I attempted to achieve best results by implementing and evaluating two classifiers. One is to use logistic regression to classify images with different image feature extraction techniques. The second approach is to use convolutional neural network (CNN), which shows incredibly better performance than logistic regression model.\n",
    "\n",
    "### PS: I don't display the data images due to the copyright of Kaggle.\n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style=\"text-align:center\"><span style=\"color:blue; font-family:Times New Roman; font-size:3em;\"> Data Preprocessing </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import mahotas as mh\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "import cv2\n",
    "from cv2 import imread\n",
    "from cv2 import imshow\n",
    "from cv2 import resize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataframe(folder, imageformat):\n",
    "    Class=[]\n",
    "    ID=[]\n",
    "    filename=[]\n",
    "    for file_name in glob.glob('{}*'.format(folder)):\n",
    "        label_name=file_name.split('\\\\')[-1]\n",
    "        image=glob.glob(folder+label_name+'/*.{}'.format(imageformat))\n",
    "        for f in image:\n",
    "            id_label=f.split('\\\\')[-1][:-4]\n",
    "            ID.append(id_label)\n",
    "            Class.append(label_name)\n",
    "            filename.append(f)\n",
    "    df=DataFrame(data={'filename':filename, 'ID':ID, 'Class':Class})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=make_dataframe('train/','jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(df):\n",
    "    fig, ax=plt.subplots(nrows=3,ncols=3, figsize=(12,12))\n",
    "    for j in range(0,3):\n",
    "        for i in range(0,3):http://localhost:8888/notebooks/Data%20science%20portfolio/Cervix%20type%20classification/Cervical%20image%20analysis_preprocessing-github.ipynb#\n",
    "            files_for_each_class=df[df['Class']==df['Class'].unique()[i]]['filename'] \n",
    "            ax[j,i].imshow(mh.imread(files_for_each_class.values[j]))\n",
    "            ax[j,i].set_title(files_for_each_class.values[j].split('/')[1])\n",
    "            ax[j,i].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\">   The images are downloaded from Kaggle. The representative images of three cervix types are shown below. \n",
    "\n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_images(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left\"><span style=\"color:red; font-family:Times New Roman; font-size:1.5em;\">  \n",
    "Preprocessing step 1 : Cervix segmentation </span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Preprocessing is the first step of image analysis. The purpose of preprocessing is to make recognition of image objects easier and improve the quality of the image being processed. In order to enhance accuracy of cervix classification, I want to remove the speculums that might be shown in images. Since the pixels on a speculum location are darker than those on a cervix area, thresholding is used to convert a color image to a bi-level image by using an optimal threshold. Each pixel in a bi-level image is set to either 0 or 1. After thresholding, an image is chopped by finding the largest rectangular area with pixel value 1. Therefore, those pixels that contain a cervix object is extracted and separated from a speculum.  The algorithm theory of fining the largest rectangular area in a bi-level image is explained in https://www.youtube.com/watch?v=g8bSdXCG-lA, https://www.youtube.com/watch?v=VNbkzsnllsU.\n",
    "\n",
    "\n",
    "\n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chop_image(image):\n",
    "    maxarea=0\n",
    "    row=np.zeros(image.shape[1])\n",
    "    maxarea_row_positions=[0,0]\n",
    "    maxarea_column_positions=[0,0]\n",
    "    column_position=[]\n",
    "    length=[]\n",
    "    for i in range(image.shape[0]):#loop over each row\n",
    "        for j in range(image.shape[1]):#loop over each column in a row\n",
    "            if image[i,j]>0:\n",
    "                row[j]+=image[i,j]\n",
    "            elif image[i,j]==0:\n",
    "                row[j]=0\n",
    "    \n",
    "        count=0\n",
    "        for j in range(image.shape[1]):\n",
    "            if row[j] > 0:\n",
    "                if maxarea < row[j]:\n",
    "                    maxarea = row[j]\n",
    "                    maxarea_column_positions=[j,j]\n",
    "                    maxarea_row_positions=[i-row[j]+1, i]\n",
    "                column_position.append(j)\n",
    "                length.append(row[j])\n",
    "                count+=1\n",
    "                if j == image.shape[1]-1:\n",
    "                    if maxarea < count*np.min(length):\n",
    "                        maxarea = count*np.min(length)\n",
    "                        maxarea_column_positions=[column_position[0],column_position[-1] ]\n",
    "                        maxarea_row_positions=[i-np.min(length)+1, i]\n",
    "                    column_position=[]\n",
    "                    length=[]\n",
    "            elif row[j] == 0:\n",
    "                if row[j-1]>0 and j!=0:\n",
    "                    if maxarea < count*np.min(length):\n",
    "                        maxarea = count*np.min(length)\n",
    "                        maxarea_column_positions=[column_position[0],column_position[-1] ]\n",
    "                        maxarea_row_positions=[i-np.min(length)+1, i]\n",
    "                    count=0\n",
    "                    column_position=[]\n",
    "                    length=[]\n",
    "    return maxarea_row_positions, maxarea_column_positions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(im, xmargin,ymargin,a):\n",
    "    img = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    _, thresh =cv2.threshold(gray, (gray.max())/a, gray.max(), cv2.THRESH_BINARY)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(thresh.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "    main_contour = sorted(contours, key = cv2.contourArea, reverse = True)[0]\n",
    "    ff = np.zeros((gray.shape[0],gray.shape[1]), 'uint8') \n",
    "    cv2.drawContours(ff, main_contour,-1,1, 5)\n",
    "    ff_mask = np.zeros((gray.shape[0]+2,gray.shape[1]+2), 'uint8')\n",
    "    cv2.floodFill(ff, ff_mask, (int(gray.shape[1]/2), int(gray.shape[0]/2)), 1)\n",
    "\n",
    "    [y1,y2], [x1,x2]=chop_image(ff)\n",
    "    return img, img[np.where(y1-ymargin>0, y1-ymargin,0):y2+ymargin,\\\n",
    "                    np.where(x1-xmargin>0, x1-xmargin,0):np.where(x2+xmargin<img.shape[1], x2+xmargin,img.shape[1]-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    classtype=i\n",
    "    for j in range(0,len(df[df['Class']==df['Class'].unique()[i]])):\n",
    "        filenumber=j\n",
    "        files_for_each_class=df[df['Class']==df['Class'].unique()[classtype]]['filename']\n",
    "        im=cv2.imread(files_for_each_class.values[filenumber])\n",
    "        original, altered=preprocess_image(im, 0,0,3)#(horizontal, vertical)\n",
    "        alter_image=altered\n",
    "        mh.imsave(\"train_cut/{}/{}.jpg\".format(df['Class'].unique()[classtype],\\\n",
    "                                               df[df['Class']==df['Class'].unique()[classtype]]['ID'].values[filenumber]),\\\n",
    "                  alter_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> The representative chopped images of three cervix types are shown below. A cervix object is well extracted from a source image. \n",
    "\n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cut=make_dataframe('train_cut/','jpg')\n",
    "plot_images(df_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left\"><span style=\"color:red; font-family:Times New Roman; font-size:1.5em;\">  \n",
    "Preprocessing step 2 : Resize image to 256 x 256 for logistic regression modeling</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "The second step of image preprocessing is to ensure the image size consistent across the dataset. Since the chopped images generated from the first preprocessing step have different sizes, I use a two-step approach for resizing the chopped images. First, I resize images by fixing either width or height and preserving the original aspect ratio. If the width of an image is shorter (longer) than the height, the width size (height size) is normalized to 256. This is done by the function ‘scale_colorimage_step1’. After that, the image chopping is applied to a longer dimension of an image so that the image size becomes 256 x 256. The function ‘scale_colorimage_step2’ tries to locate a cervix opening and chop an image along a longer dimension of the image in a such way that critical cervix pixels can be preserved. The final resized 256 x 256 images are saved through the function ‘scale_colorimage_step3’. \n",
    " \n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_colorimage_step1(im):\n",
    "    if im.shape[0] > im.shape[1]:\n",
    "        scaled_size = (256, int(im.shape[0]*256/im.shape[1]))\n",
    "    else:\n",
    "        scaled_size = (int(im.shape[1]*256/im.shape[0]),256)\n",
    "    scaled_im = cv2.resize(im, dsize=scaled_size) \n",
    "    return scaled_im\n",
    "\n",
    "def scale_colorimage_step2(scaled_im,df_cut):\n",
    "    ratio=3\n",
    "    if scaled_im.shape[0] < scaled_im.shape[1]:\n",
    "        length=len(scaled_im.sum(0))\n",
    "        profile=scaled_im.sum(0)[length/ratio:-length/ratio]\n",
    "    else:\n",
    "        length=len(scaled_im.sum(1))\n",
    "        profile=scaled_im.sum(1)[length/ratio:-length/ratio]\n",
    "    min_point=np.argmin(profile)+length/ratio\n",
    "    if (min_point >= 128) & (length-min_point >= 128):\n",
    "        end1=min_point-128\n",
    "        end2=min_point+128\n",
    "    elif (min_point > 128) & (length-min_point <= 128):\n",
    "        end1=length-256\n",
    "        end2=length\n",
    "    elif (min_point <= 128) & (length-min_point > 128):\n",
    "        end1=0\n",
    "        end2=256\n",
    "    return end1, end2\n",
    "\n",
    "def scale_colorimage_step3(scaled_im,df_cut,end1, end2,classtype,filenumber):  \n",
    "    if scaled_im.shape[0] < scaled_im.shape[1]:\n",
    "        squared_image=scaled_im[:,end1:end2]\n",
    "    else:\n",
    "        squared_image=scaled_im[end1:end2,:]\n",
    "        \n",
    "    mh.imsave(\"train_cut_colorscaled/{}/{}.png\".format(df_cut['Class'].unique()[classtype],\\\n",
    "                                           df_cut[df_cut['Class']==df_cut['Class'].unique()[classtype]]['ID'].values[filenumber]), \\\n",
    "                squared_image)\n",
    "    return squared_image\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    print 'This is Type_%d' %i\n",
    "    classtype=i\n",
    "    for j in range(0,len(df_cut[df_cut['Class']==df_cut['Class'].unique()[i]])):\n",
    "        print j\n",
    "        filenumber=j\n",
    "        files_for_each_class=df_cut[df_cut['Class']==df_cut['Class'].unique()[classtype]]['filename']       \n",
    "        im=mh.imread(files_for_each_class.values[j],as_grey=True)\n",
    "        scaled_im=scale_colorimage_step1(im)\n",
    "        end1, end2=scale_colorimage_step2(scaled_im,df_cut)\n",
    "        im=mh.imread(files_for_each_class.values[j])\n",
    "        scaled_im=scale_colorimage_step1(im)\n",
    "        squared_image=scale_colorimage_step3(scaled_im,df_cut,end1, end2,classtype,filenumber)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> The representative resized 256x256 images of three cervix types are shown below.\n",
    "\n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cut_colorscaled=make_dataframe('train_cut_colorscaled/','png')\n",
    "plot_images(df_cut_colorscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> After filtering out some blurry or empty images, I have 1409 training images. The number of images for each cervix type is listed below. \n",
    "\n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample number in the training set : 1409\n",
      "The numbers of Type 1 in the training set : 754\n",
      "The numbers of Type 2 in the training set : 239\n",
      "The numbers of Type 3 in the training set : 416\n"
     ]
    }
   ],
   "source": [
    "print 'Total sample number in the training set : %d' %(len(df_cut_colorscaled))\n",
    "print 'The numbers of Type 1 in the training set : %d' %(len(df_cut_colorscaled[df_cut_colorscaled['Class']==df_cut_colorscaled['Class'].unique()[0]]))\n",
    "print 'The numbers of Type 2 in the training set : %d' %(len(df_cut_colorscaled[df_cut_colorscaled['Class']==df_cut_colorscaled['Class'].unique()[1]]))\n",
    "print 'The numbers of Type 3 in the training set : %d' %(len(df_cut_colorscaled[df_cut_colorscaled['Class']==df_cut_colorscaled['Class'].unique()[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left\"><span style=\"color:red; font-family:Times New Roman; font-size:1.5em;\">  \n",
    "Preprocessing step 3 : Label Conversion </span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> The labels for three cervix types are converted by LabelEncoder. The label 0 corresponds to the Type 1, 1 for the Type 2 and 2 for the Type 3. \n",
    "<br \\>\n",
    "The information of all image file names, file ID, cervix types and class labels is stored in the ‘df_cut_colorscaled’ dataframe. The sample rows of the dataframe have been randomly shuffled and saved in a CSV file ('df_cut_colorscaled.csv') for logistic regression modeling.\n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "df_cut_colorscaled=df_cut_colorscaled.reindex(np.random.permutation(df_cut_colorscaled.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelencoder=LabelEncoder()\n",
    "df_cut_colorscaled['label']=labelencoder.fit_transform(df_cut_colorscaled['Class'].values)\n",
    "df_cut_colorscaled=df_cut_colorscaled.reset_index(drop=True)\n",
    "y_train_colorscaled=df_cut_colorscaled['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cut_colorscaled.to_csv('df_cut_colorscaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left\"><span style=\"color:red; font-family:Times New Roman; font-size:1.5em;\">  \n",
    "Preprocessing step 4 : Resize image to 32 x 32 for CNN modeling </span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> \n",
    "Using 256×256 images makes the training quite slow for convolutional neural network (CNN). In order to make computation fast and also produce good results, I decide to resize 256×256 images further to 32×32 images for the trade-off between computation speed and accuracy. After resizing, each image input data is expressed as a 3-dimensional matrix of channels x width x height, e.g. 3x32x32. The three channels represent the red, green and blue pixel values. I normalized the 8-bit RGB pixel values  to the range 0 and 1 by dividing each value by the maximum of 255. \n",
    "The image input data in the training dataset is saved in ‘x_train_32.pkl’. The output variable is saved in 'y_train.pkl'. \n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_resize_256_to_32(df):\n",
    "    filename=[]\n",
    "    x_data=[]\n",
    "    for f in df['filename']:\n",
    "        image=mh.imread(f)\n",
    "        if image.shape != (256L, 256L, 3L):\n",
    "            print f   \n",
    "        resized = cv2.resize(image, (32, 32))\n",
    "        x_data.append(resized)\n",
    "        filename.append(f.split('\\\\')[-1][:-4] + '.jpg')\n",
    "    x_data=np.array(x_data, dtype=np.uint8)\n",
    "    x_data=x_data.transpose((0, 3,1,2)).astype('float32')/255\n",
    "    print x_data.shape\n",
    "    return x_data, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1409L, 3L, 32L, 32L)\n"
     ]
    }
   ],
   "source": [
    "x_traindata, filename_traindata=image_resize_256_to_32(df_cut_colorscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_train.pkl']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(x_traindata,'x_train_32.pkl')\n",
    "joblib.dump(y_train_colorscaled,'y_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left\"><span style=\"color:red; font-family:Times New Roman; font-size:1.5em;\">  \n",
    "Preprocessing step 5 : prepare the test dataset </span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> \n",
    "The test image dataset is downloaded from Kaggle. The test data has 512 images. The test images are preprocessed through chopping and resizing into 32x32 images for CNN prediction. The image input data in the test dataset is saved in 'x_test_32.pkl'. The file names is saved in 'x_test_ID.pkl'.  \n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataframe_for_testdata(folder):\n",
    "    ID=[]\n",
    "    filename=[]\n",
    "    for file_name in glob.glob('{}*'.format(folder)):\n",
    "        id_label=file_name.split('\\\\')[-1][:-4]\n",
    "        ID.append(id_label)\n",
    "        filename.append(file_name)\n",
    "    df_test=DataFrame(data={'filename':filename, 'ID':ID})\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample number in the test set : 512\n"
     ]
    }
   ],
   "source": [
    "df_test=make_dataframe_for_testdata('test/')\n",
    "print 'Total sample number in the test set : %d' %(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filenumber in range(0,len(df_test)):\n",
    "    files=df_test['filename']   \n",
    "    im=cv2.imread(files.values[filenumber])\n",
    "    original, altered=preprocess_image(im, 0,0,3)#(horizontal, vertical)\n",
    "    alter_image=altered\n",
    "    mh.imsave(\"test_cut/{}.jpg\".format(df_test['ID'].values[filenumber]),alter_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_cut=make_dataframe_for_testdata('test_cut/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filenumber in range(0,len(df_test_cut)):\n",
    "    files=df_test_cut['filename']       \n",
    "    im=mh.imread(files.values[j],as_grey=True)\n",
    "    scaled_im=scale_colorimage_step1(im)\n",
    "    end1, end2=scale_colorimage_step2(scaled_im,df_test_cut)\n",
    "    im=mh.imread(files.values[j])\n",
    "    scaled_im=scale_colorimage_step1(im)\n",
    "    squared_image=scale_colorimage_step3(scaled_im,df_test_cut,end1, end2,filenumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512L, 3L, 32L, 32L)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x_test_ID.pkl']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_scaled=make_dataframe_for_testdata('test_cut_colorscaled/')\n",
    "x_testdata, filename_testdata=image_resize_256_to_32(df_test_scaled)\n",
    "joblib.dump(x_testdata,'x_test_32.pkl')\n",
    "joblib.dump(np.array(filename_testdata),'x_test_ID.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
