{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><span style=\"color:blue; font-family:Times New Roman; font-size:3em;\"> Convolutional neural network modeling </span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Convolutional neural networks have unique architecture and are reportedly successful at image recognition. Because my laptop always got frozen while training a CNN, I decide to use Google Cloud Datalab which is an interactive tool for Jupyter notebooks. The Datalab is connected to a Google Compute Engine Virtual Machine (VM) with 16 CPU for massive data training. The image input data and output variables for the training and test datasets are preprocessed, saved in Google Cloud Storage, and downloaded to Datalab. x_data and y_train are the input and output variables of the training dataset. x_test and x_test_ID are the input data and image IDs for the test dataset. \n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in c:\\python27\\scripts\\anaconda2\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.0.4.tar.gz (199kB)\n",
      "\u001b[K    100% |################################| 204kB 2.9MB/s \n",
      "\u001b[?25hCollecting theano (from keras)\n",
      "  Downloading Theano-0.9.0.tar.gz (3.1MB)\n",
      "\u001b[K    100% |################################| 3.1MB 396kB/s \n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from keras)\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from theano->keras)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from theano->keras)\n",
      "Building wheels for collected packages: keras, theano\n",
      "  Running setup.py bdist_wheel for keras ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/48/82/42/f06a8c03a8f95ada523a81ba723e89f059693e6ad868d09727\n",
      "  Running setup.py bdist_wheel for theano ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d5/5b/93/433299b86e3e9b25f0f600e4e4ebf18e38eb7534ea518eba13\n",
      "Successfully built keras theano\n",
      "Installing collected packages: theano, keras\n",
      "Successfully installed keras-2.0.4 theano-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5py==2.6\n",
      "  Downloading h5py-2.6.0-1-cp27-cp27mu-manylinux1_x86_64.whl (4.2MB)\n",
      "\u001b[K    100% |################################| 4.2MB 310kB/s \n",
      "\u001b[?25hCollecting numpy>=1.6.1 (from h5py==2.6)\n",
      "  Downloading numpy-1.12.1-cp27-cp27mu-manylinux1_x86_64.whl (16.5MB)\n",
      "\u001b[K    100% |################################| 16.5MB 65kB/s \n",
      "\u001b[?25hRequirement already up-to-date: six in /usr/local/lib/python2.7/dist-packages (from h5py==2.6)\n",
      "Installing collected packages: numpy, h5py\n",
      "  Found existing installation: numpy 1.11.2\n",
      "    Uninstalling numpy-1.11.2:\n",
      "      Successfully uninstalled numpy-1.11.2\n",
      "Successfully installed h5py-2.6.0 numpy-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U h5py==2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sed -i.bak '/run_tests/d' /usr/local/lib/python2.7/dist-packages/h5py/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from StringIO import StringIO\n",
    "from io import BytesIO \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D \n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%storage read --object gs://image1234/x_data_32.pkl --variable x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1409, 3, 32, 32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data=joblib.load(BytesIO(x_data), mmap_mode='c')\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%storage read --object gs://image1234/y_train_colorscaled.pkl --variable y_train_colorscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1409,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=joblib.load(BytesIO(y_train_colorscaled), mmap_mode='c')\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "I use k-Fold Cross Validation to estimate the performance of a model on unseen data. The training dataset is split into 10 subsets. Each time, a model will train on all subsets except one which is held out as validation set. Training the model 10 times on 90% of the data and testing on 10% can provide a robust estimate of model performance.\n",
    "\n",
    "\n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kfolds=KFold(n_splits=10, shuffle=True,random_state=0)\n",
    "trainindex=[]\n",
    "validindex=[]\n",
    "for train_index, valid_index in kfolds.split(x_data, y_train):\n",
    "    trainindex.append(train_index)\n",
    "    validindex.append(valid_index)\n",
    "trainindex=np.array(trainindex)\n",
    "validindex=np.array(validindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "The network structure consists of at least 3 convolutional layers. Additional 3 convolutional layers can be added. Each convolutional layer has a filter size of 3 x 3 and a rectifier activation function. \n",
    "Every one or two convolutional layers will be followed by a max pool layer with a window size of 2Ã—2.\n",
    "Afterwards, the output of the third pooling layer is flattened to 1D as the Flatten layer, and passed through two fully connected Dense layers. Better performance is achieved using the tanh activation function in the first dense layer. \n",
    "The last dense layer has the same number of outputs as the number of the class types and a softmax activation is used for purposes of probabilistic classification.\n",
    "<br\\>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Two Dropout layers are applied after the third pooling layer, and after the first Dense layer. Dropout is able to randomly select nodes to be dropped-out with a given probability at each weight update cycle, which can achieve better generalization error and is less likely to overfit the training data. \n",
    "<br\\>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "After the model is defined, I compile the model by specifying  'sparse_categorical_crossentropy' as the chosen loss function to evaluate a set of weights. I use AdaMax as an optimizer to search through different weights for the network because it performs better than other optimization methods.\n",
    "<br\\>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Due to a limited amount data for deep learning training, I apply image augmentations to artificially increase the size of the training set with new transformed images. A number of random transformations are applied to the initial data by zooming, rotating, shifting or shearing images to prevent overfitting and improving the model generalization error. \n",
    "<br\\>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "In the end, the model is fitted by specifying training data, validation data, the number of training epochs and the batch size. \n",
    "The model weights that give the best result (lowest loss in the validation set) is saved by ModelCheckpoint in the file 'weights32_testnumber.best.hdf5'. The best model weights can be reloaded for model evaluation and prediction.        \n",
    "\n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_32(x_data, y_train,trainindex, validindex, foldnumber, testnumber, c1,c2,c3,c4,c5,c6,c7,nb_epoch ):\n",
    "    X_train, X_valid = x_data[trainindex[foldnumber]], x_data[validindex[foldnumber]]\n",
    "    y_train, y_valid = y_train[trainindex[foldnumber]], y_train[validindex[foldnumber]]\n",
    "          \n",
    "    model=Sequential()\n",
    "    model.add(Convolution2D(nb_filter=c1, nb_row=3, nb_col=3, border_mode='same',\\\n",
    "                            input_shape=(3,32,32), activation='relu'))\n",
    "    if c2 != 0:\n",
    "        model.add(Convolution2D(nb_filter=c2, nb_row=3, nb_col=3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    \n",
    "    ############### \n",
    "    model.add(Convolution2D(nb_filter=c3, nb_row=3, nb_col=3, border_mode='same', \\\n",
    "                            activation='relu'))\n",
    "    if c4 != 0:\n",
    "        model.add(Convolution2D(nb_filter=c4, nb_row=3, nb_col=3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "   \n",
    "    ###############          \n",
    "    model.add(Convolution2D(nb_filter=c5, nb_row=3, nb_col=3, border_mode='same', activation='relu'))\n",
    "    if c6 != 0:\n",
    "        model.add(Convolution2D(nb_filter=c6, nb_row=3, nb_col=3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    \n",
    "    ###############\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(c7, init='he_normal', activation = 'tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, init='he_normal', activation = 'softmax'))\n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adamax',#optimizer='adadelta', 'adamax','rmsprop','adam',sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    datagen = ImageDataGenerator(rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2,\n",
    "                                 zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')   \n",
    "    filepath=\"weights32_{}.best.hdf5\".format(testnumber)\n",
    "    saveBestModel = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')  \n",
    "     \n",
    "    model.fit_generator(datagen.flow(X_train, y_train, batch_size=500, shuffle=True), \\\n",
    "                        nb_epoch=nb_epoch, samples_per_epoch=len(X_train), verbose=0, \\\n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        callbacks=[saveBestModel])\n",
    "    model.load_weights(filepath)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adamax',\n",
    "                  metrics=['accuracy'])\n",
    "    scores_valid = model.evaluate(X_valid, y_valid,verbose=0)\n",
    "    scores_train = model.evaluate(X_train, y_train,verbose=0)\n",
    "    print(\"%s in the train set: %.6f\" % (model.metrics_names[0], scores_train[0]))\n",
    "    print(\"%s in the train set: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100)) \n",
    "    print(\"%s in the validation set: %.6f\" % (model.metrics_names[0], scores_valid[0]))\n",
    "    print(\"%s in the validation set: %.2f%%\" % (model.metrics_names[1], scores_valid[1]*100))    \n",
    "    return scores_train[0], scores_valid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "10-fold train/validation combinations are iterated and trained in a CNN model. The model performance is evaluated by averaging the loss values in the 10-fold validation sets. For an example shown below, this CNN model consists of 4 convolutional layers with the filter numbers of 16, 16, 16 and 32. The output neurons of the first dense layer is 32. The average validation loss for this model is 0.77649 +/- 0.03402, which is much better than a logistic regression model presented earlier.\n",
    "\n",
    "\n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 0-th fold : \n",
      "loss in the train set: 0.713650\n",
      "acc in the train set: 68.45%\n",
      "loss in the validation set: 0.795650\n",
      "acc in the validation set: 60.28%\n",
      "This is 1-th fold : \n",
      "loss in the train set: 0.733433\n",
      "acc in the train set: 67.35%\n",
      "loss in the validation set: 0.811755\n",
      "acc in the validation set: 65.96%\n",
      "This is 2-th fold : \n",
      "loss in the train set: 0.772045\n",
      "acc in the train set: 65.46%\n",
      "loss in the validation set: 0.744944\n",
      "acc in the validation set: 69.50%\n",
      "This is 3-th fold : \n",
      "loss in the train set: 0.743023\n",
      "acc in the train set: 67.74%\n",
      "loss in the validation set: 0.802167\n",
      "acc in the validation set: 60.99%\n",
      "This is 4-th fold : \n",
      "loss in the train set: 0.697524\n",
      "acc in the train set: 68.85%\n",
      "loss in the validation set: 0.791737\n",
      "acc in the validation set: 63.83%\n",
      "This is 5-th fold : \n",
      "loss in the train set: 0.710163\n",
      "acc in the train set: 68.53%\n",
      "loss in the validation set: 0.726430\n",
      "acc in the validation set: 69.50%\n",
      "This is 6-th fold : \n",
      "loss in the train set: 0.713938\n",
      "acc in the train set: 68.06%\n",
      "loss in the validation set: 0.723872\n",
      "acc in the validation set: 66.67%\n",
      "This is 7-th fold : \n",
      "loss in the train set: 0.719586\n",
      "acc in the train set: 68.38%\n",
      "loss in the validation set: 0.796617\n",
      "acc in the validation set: 64.54%\n",
      "This is 8-th fold : \n",
      "loss in the train set: 0.725420\n",
      "acc in the train set: 67.82%\n",
      "loss in the validation set: 0.819178\n",
      "acc in the validation set: 60.99%\n",
      "This is 9-th fold : \n",
      "loss in the train set: 0.781765\n",
      "acc in the train set: 64.93%\n",
      "loss in the validation set: 0.752595\n",
      "acc in the validation set: 67.86%\n"
     ]
    }
   ],
   "source": [
    "train_scores=[]\n",
    "valid_scores=[]\n",
    "for k in range(0,10):\n",
    "    print 'This is %d-th fold : ' %k\n",
    "    scores_train, scores_valid= cnn_32(x_data, y_train,trainindex, validindex, k, k,  16,16,16,0,32,0,32,500)\n",
    "    train_scores.append(scores_train)\n",
    "     valid_scores.append(scores_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average loss in the train set is 0.73105 +/- 0.02592\n",
      "The average loss in the validation set is 0.77649 +/- 0.03402\n"
     ]
    }
   ],
   "source": [
    "print 'The average loss in the train set is %.5f +/- %.5f' %(np.mean(train_scores),np.std(train_scores))\n",
    "print 'The average loss in the validation set is %.5f +/- %.5f' %(np.mean(valid_scores),np.std(valid_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "After being through countless trials with different combinations of convolutional layer numbers, filter numbers in convolutional layers, output numbers in the first dense layer, and the epoch numbers, a best model is selected based on its average loss score. Finally, the test dataset is feed into the best model which generates 10 predictions via the iterations over 10-fold cross validations. The final prediction for the test set is obtained by averaging all 10 predictions in order to reduce overfitting. \n",
    "\n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%storage read --object gs://image1234/x_test_32.pkl --variable x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%storage read --object gs://image1234/x_test_ID.pkl --variable x_test_ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 3, 32, 32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test=joblib.load(BytesIO(x_test), mmap_mode='c') \n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test_ID =joblib.load(BytesIO(x_test_ID), mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_predict(x_test, x_data, y_train,trainindex, validindex, foldnumber, testnumber, c1,c2,c3,c4,c5,c6,c7):\n",
    "    X_train, X_valid = x_data[trainindex[foldnumber]], x_data[validindex[foldnumber]]\n",
    "    y_train, y_valid = y_train[trainindex[foldnumber]], y_train[validindex[foldnumber]]\n",
    "          \n",
    "    model=Sequential()\n",
    "    model.add(Convolution2D(nb_filter=c1, nb_row=3, nb_col=3, border_mode='same',\\\n",
    "                            input_shape=(3,32,32), activation='relu'))\n",
    "    if c2 != 0:\n",
    "        model.add(Convolution2D(nb_filter=c2, nb_row=3, nb_col=3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    \n",
    "    ############### \n",
    "    model.add(Convolution2D(nb_filter=c3, nb_row=3, nb_col=3, border_mode='same', \\\n",
    "                            activation='relu'))\n",
    "    if c4 != 0:\n",
    "        model.add(Convolution2D(nb_filter=c4, nb_row=3, nb_col=3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "   \n",
    "    ###############          \n",
    "    model.add(Convolution2D(nb_filter=c5, nb_row=3, nb_col=3, border_mode='same', activation='relu'))\n",
    "    if c6 != 0:\n",
    "        model.add(Convolution2D(nb_filter=c6, nb_row=3, nb_col=3, border_mode='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    \n",
    "    ###############\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(c7, init='he_normal', activation = 'tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, init='he_normal', activation = 'softmax'))\n",
    "  \n",
    "    ###############\n",
    "    filepath=\"weights32_{}.best.hdf5\".format(testnumber)\n",
    "    model.load_weights(filepath)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adamax',\n",
    "                  metrics=['accuracy'])\n",
    "    scores = model.evaluate(X_valid, y_valid,verbose=0)\n",
    "    y_pred = model.predict_proba(x_test)\n",
    "   \n",
    "    print(\"%s in the validation set: %.6f\" % (model.metrics_names[0], scores[0]))\n",
    "    print(\"%s in the validation set: %.2f%%\" % (model.metrics_names[1], scores[1]*100))    \n",
    "    return y_pred\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/512 [===========================>..] - ETA: 0sloss in the validation set: 0.795650\n",
      "acc in the validation set: 60.28%\n",
      "480/512 [===========================>..] - ETA: 0sloss in the validation set: 0.811755\n",
      "acc in the validation set: 65.96%\n",
      "480/512 [===========================>..] - ETA: 0sloss in the validation set: 0.744944\n",
      "acc in the validation set: 69.50%\n",
      "480/512 [===========================>..] - ETA: 0sloss in the validation set: 0.802167\n",
      "acc in the validation set: 60.99%\n",
      "480/512 [===========================>..] - ETA: 0sloss in the validation set: 0.791737\n",
      "acc in the validation set: 63.83%\n",
      "480/512 [===========================>..] - ETA: 0sloss in the validation set: 0.726430\n",
      "acc in the validation set: 69.50%\n",
      "480/512 [===========================>..] - ETA: 0sloss in the validation set: 0.723872\n",
      "acc in the validation set: 66.67%\n",
      "480/512 [===========================>..] - ETA: 0sloss in the validation set: 0.796617\n",
      "acc in the validation set: 64.54%\n",
      "480/512 [===========================>..] - ETA: 0sloss in the validation set: 0.819178\n",
      "acc in the validation set: 60.99%\n",
      "480/512 [===========================>..] - ETA: 0sloss in the validation set: 0.752595\n",
      "acc in the validation set: 67.86%\n"
     ]
    }
   ],
   "source": [
    "y_pred_all=[]\n",
    "for k in range(0,10):\n",
    "    y_pred=cnn_predict(x_test, x_data, y_train,trainindex, validindex, k, k,  16,16,16,0,32,0,32)\n",
    "    y_pred_all.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_all=np.array(y_pred_all)\n",
    "y_pred_average=np.average(y_pred_all, axis=0)\n",
    "df = pd.DataFrame(y_pred_average, columns=['Type_1','Type_2','Type_3'])\n",
    "df['image_name'] = x_test_ID\n",
    "df = df.reindex(columns=['image_name','Type_1','Type_2','Type_3'])\n",
    "df.to_csv('submission_a1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><span style=\"color:blue; font-family:Times New Roman; font-size:3em;\"> Conclusion  </span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:justify\"><span style=\"color:black; font-family:Times New Roman; font-size:1.5em;line-height:1.4em;\"> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "In this project, I developed two learning algorithms to classify cervix types.  A logistic regression model combined with different image feature extraction techniques is simple to implement and low computation cost. However, this classifier only can achieve a Log Loss  value of 0.86822. Convolutional neural networks are very expensive to train but shows superior performance. I have successfully implemented a multi-layer CNN in Keras with the combination of Dropout, image augmentations and k-fold cross validation in order to reduce overfitting and log-loss value . The best result of a log loss from CNN is 0.752 in the validation set and 0.758 in the test set, leading to the ranking of top 12% in the Kaggle competition â€˜Cervical Cancer Screeningâ€™.\n",
    " </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
